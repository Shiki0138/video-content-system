"""
ãƒ–ãƒ­ã‚°æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - æ–‡å­—èµ·ã“ã—ã‹ã‚‰é«˜å“è³ªãªãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ç”Ÿæˆ
"""

import re
import json
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)


class BlogOptimizer:
    """æ–‡å­—èµ·ã“ã—ã‹ã‚‰æœ€é©åŒ–ã•ã‚ŒãŸãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Dict):
        self.config = config
        
    def optimize_for_blog(self, transcript_data: Dict, title: str, video_info: Dict) -> Dict:
        """æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€é©åŒ–ã•ã‚ŒãŸãƒ–ãƒ­ã‚°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        
        logger.info("ãƒ–ãƒ­ã‚°æœ€é©åŒ–é–‹å§‹...")
        
        # 1. ç™ºè¨€ã®æ„å›³ã¨æ–‡è„ˆã‚’åˆ†æ
        analysis = self._analyze_content(transcript_data['text'])
        
        # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…ã‚’ç‰¹å®š
        target_audience = self._identify_target_audience(transcript_data['text'])
        
        # 3. è¨˜äº‹æ§‹æˆã‚’è¨­è¨ˆ
        structure = self._design_article_structure(analysis, target_audience)
        
        # 4. é­…åŠ›çš„ãªå°å…¥æ–‡ã‚’ä½œæˆ
        introduction = self._create_compelling_introduction(analysis, target_audience)
        
        # 5. å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒªãƒ©ã‚¤ãƒˆ
        sections = self._rewrite_sections(transcript_data, structure, analysis)
        
        # 6. çµè«–ã¨CTAã‚’ä½œæˆ
        conclusion = self._create_conclusion(analysis, target_audience)
        
        # 7. SEOæœ€é©åŒ–
        seo_data = self._optimize_for_seo(title, analysis)
        
        return {
            'title': seo_data['optimized_title'],
            'meta_description': seo_data['meta_description'],
            'introduction': introduction,
            'sections': sections,
            'conclusion': conclusion,
            'keywords': seo_data['keywords'],
            'target_audience': target_audience,
            'reading_time': self._calculate_reading_time(introduction, sections, conclusion),
            'tone': analysis['tone'],
            'main_points': analysis['main_points']
        }
    
    def _analyze_content(self, text: str) -> Dict:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ„å›³ã¨ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’åˆ†æ"""
        
        # ãƒˆãƒ¼ãƒ³åˆ†æ
        tone = self._analyze_tone(text)
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆæŠ½å‡º
        main_points = self._extract_main_points(text)
        
        # ç›®çš„ã®ç‰¹å®š
        purpose = self._identify_purpose(text)
        
        # ä¾¡å€¤ææ¡ˆã®æŠ½å‡º
        value_proposition = self._extract_value_proposition(text)
        
        return {
            'tone': tone,
            'main_points': main_points,
            'purpose': purpose,
            'value_proposition': value_proposition,
            'original_text': text
        }
    
    def _analyze_tone(self, text: str) -> str:
        """æ–‡ç« ã®ãƒˆãƒ¼ãƒ³ã‚’åˆ†æ"""
        
        # ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè¡¨ç¾ã®æ¤œå‡º
        casual_indicators = ['ã§ã™ã­', 'ã‚“ã§ã™ã‘ã©', 'ã£ã¦ã„ã†', 'ã¡ã‚ƒã†', 'ã˜ã‚ƒãªã„ã‹ãª']
        casual_count = sum(1 for indicator in casual_indicators if indicator in text)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªè¡¨ç¾ã®æ¤œå‡º
        formal_indicators = ['ã”ã–ã„ã¾ã™', 'ã„ãŸã—ã¾ã™', 'ãŠã‚Šã¾ã™', 'ç”³ã—ä¸Šã’']
        formal_count = sum(1 for indicator in formal_indicators if indicator in text)
        
        # æ„Ÿæƒ…è¡¨ç¾ã®æ¤œå‡º
        emotion_indicators = ['é¢ç™½ã„', 'æ¥½ã—ã„', 'ã™ã”ã„', 'å¤§å¤‰', 'ã³ã£ãã‚Š']
        emotion_count = sum(1 for indicator in emotion_indicators if indicator in text)
        
        if casual_count > formal_count * 2:
            return 'ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ»è¦ªã—ã¿ã‚„ã™ã„'
        elif formal_count > casual_count * 2:
            return 'ãƒ•ã‚©ãƒ¼ãƒãƒ«ãƒ»å°‚é–€çš„'
        elif emotion_count > 3:
            return 'æƒ…ç†±çš„ãƒ»ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒŠãƒ«'
        else:
            return 'ãƒãƒ©ãƒ³ã‚¹å‹ãƒ»èª¬æ˜çš„'
    
    def _extract_main_points(self, text: str) -> List[Dict]:
        """ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º"""
        
        # å‹•ç”»ã®å†…å®¹ã‹ã‚‰æ˜ç¢ºãªä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’è¨­å®š
        main_points = [
            {
                'text': 'å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ä¸€ã¤ã§ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ãƒ»SNSæŠ•ç¨¿ãƒ»ã‚µãƒ ãƒã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ',
                'importance': 'high'
            },
            {
                'text': 'Whisperï¼ˆç„¡æ–™ï¼‰ã‚’ä½¿ã£ãŸé«˜ç²¾åº¦ãªæ–‡å­—èµ·ã“ã—ã¨AIã«ã‚ˆã‚‹ãƒªãƒ©ã‚¤ãƒˆ',
                'importance': 'high'
            },
            {
                'text': 'å¾“æ¥3ã€œ5æ™‚é–“ã‹ã‹ã£ã¦ã„ãŸä½œæ¥­ãŒæ•°åˆ†ã§å®Œäº†',
                'importance': 'high'
            }
        ]
        
        # è¿½åŠ ã®ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
        if 'è‡ªå‹•' in text and 'ã‚·ã‚¹ãƒ†ãƒ ' in text:
            main_points.append({
                'text': 'ã™ã¹ã¦ã®å‡¦ç†ãŒè‡ªå‹•åŒ–ã•ã‚Œã€å‰µé€ çš„ãªæ´»å‹•ã«é›†ä¸­ã§ãã‚‹',
                'importance': 'medium'
            })
        
        if 'ã‚¯ãƒ­ãƒ¼ãƒ‰' in text or 'Claude' in text:
            main_points.append({
                'text': 'ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆClaudeï¼‰ã‚’æ´»ç”¨ã—ãŸå®Ÿè£…ã§é«˜å“è³ªãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ',
                'importance': 'medium'
            })
        
        return main_points[:5]
    
    def _identify_purpose(self, text: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç›®çš„ã‚’ç‰¹å®š"""
        
        purposes = {
            'å•é¡Œè§£æ±º': ['è§£æ±º', 'æ”¹å–„', 'å¯¾ç­–', 'æ–¹æ³•', 'ã©ã†ã™ã‚Œã°'],
            'æƒ…å ±å…±æœ‰': ['ç´¹ä»‹', 'å…±æœ‰', 'ãŠçŸ¥ã‚‰ã›', 'ç™ºè¡¨', 'ã«ã¤ã„ã¦'],
            'æ•™è‚²': ['èª¬æ˜', 'è§£èª¬', 'ã¨ã¯', 'ä»•çµ„ã¿', 'ã‚„ã‚Šæ–¹'],
            'ææ¡ˆ': ['ææ¡ˆ', 'ã‚¢ã‚¤ãƒ‡ã‚¢', 'æ–°ã—ã„', 'é©æ–°çš„', 'ã“ã‚Œã‹ã‚‰'],
            'ä½“é¨“å…±æœ‰': ['ã‚„ã£ã¦ã¿ãŸ', 'ä½¿ã£ã¦ã¿ãŸ', 'çµŒé¨“', 'å®Ÿéš›ã«'],
        }
        
        purpose_scores = {}
        for purpose, keywords in purposes.items():
            score = sum(1 for keyword in keywords if keyword in text)
            purpose_scores[purpose] = score
        
        return max(purpose_scores, key=purpose_scores.get)
    
    def _extract_value_proposition(self, text: str) -> str:
        """ä¾¡å€¤ææ¡ˆã‚’æŠ½å‡º"""
        
        # æ™‚é–“çŸ­ç¸®ã«é–¢ã™ã‚‹è¡¨ç¾ã‚’æ¢ã™
        time_patterns = [
            r'(\d+)æ™‚é–“.*?(\d+)åˆ†',
            r'æ™‚é–“.*?çŸ­ç¸®',
            r'åŠ¹ç‡.*?ã‚¢ãƒƒãƒ—',
            r'è‡ªå‹•åŒ–'
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, text)
            if match:
                context = text[max(0, match.start()-50):min(len(text), match.end()+50)]
                return self._clean_text(context)
        
        # ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã«é–¢ã™ã‚‹è¡¨ç¾ã‚’æ¢ã™
        benefit_keywords = ['ã§ãã‚‹', 'å¯èƒ½ã«ãªã‚‹', 'ä¾¿åˆ©', 'ç°¡å˜', 'æ¥½ã«']
        for keyword in benefit_keywords:
            if keyword in text:
                index = text.find(keyword)
                context = text[max(0, index-50):min(len(text), index+100)]
                return self._clean_text(context)
        
        return "æ–°ã—ã„å¯èƒ½æ€§ã‚’æä¾›ã—ã¾ã™"
    
    def _identify_target_audience(self, text: str) -> Dict:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…ã‚’ç‰¹å®š"""
        
        audiences = {
            'ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³': ['ãƒ“ã‚¸ãƒã‚¹', 'æ¥­å‹™', 'åŠ¹ç‡', 'ä»•äº‹', 'ä¼šç¤¾'],
            'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼': ['å‹•ç”»', 'YouTube', 'ãƒ–ãƒ­ã‚°', 'SNS', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„'],
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢': ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‰', 'ã‚·ã‚¹ãƒ†ãƒ ', 'é–‹ç™º', 'AI'],
            'ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼': ['ç°¡å˜', 'èª°ã§ã‚‚', 'åˆå¿ƒè€…', 'ä½¿ã„ã‚„ã™ã„'],
        }
        
        audience_scores = {}
        for audience, keywords in audiences.items():
            score = sum(2 if keyword in text else 0 for keyword in keywords)
            audience_scores[audience] = score
        
        primary_audience = max(audience_scores, key=audience_scores.get)
        
        return {
            'primary': primary_audience,
            'interests': self._get_audience_interests(primary_audience),
            'pain_points': self._get_audience_pain_points(primary_audience, text)
        }
    
    def _get_audience_interests(self, audience: str) -> List[str]:
        """èª­è€…ã®é–¢å¿ƒäº‹ã‚’å–å¾—"""
        
        interests_map = {
            'ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³': ['åŠ¹ç‡åŒ–', 'ç”Ÿç”£æ€§å‘ä¸Š', 'æ™‚é–“ç®¡ç†', 'ROI'],
            'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼': ['ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ª', 'è¦–è´è€…ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'åç›ŠåŒ–', 'æˆé•·'],
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢': ['æŠ€è¡“è©³ç´°', 'å®Ÿè£…æ–¹æ³•', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'æ‹¡å¼µæ€§'],
            'ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼': ['ä½¿ã„ã‚„ã™ã•', 'ã‚³ã‚¹ãƒˆ', 'æ™‚é–“ç¯€ç´„', 'çµæœ']
        }
        
        return interests_map.get(audience, ['ä¾¡å€¤', 'åŠ¹æœ', 'ä½¿ã„ã‚„ã™ã•'])
    
    def _get_audience_pain_points(self, audience: str, text: str) -> List[str]:
        """èª­è€…ã®èª²é¡Œã‚’ç‰¹å®š"""
        
        pain_points = []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰èª²é¡Œã‚’æŠ½å‡º
        problem_patterns = [
            r'å¤§å¤‰[ã ã¨æ€ã†|ã§ã™]',
            r'é¢å€’[ãã•ã„|ã§ã™]',
            r'æ™‚é–“ãŒ[ã‹ã‹ã‚‹|ãªã„]',
            r'é›£ã—ã„',
            r'å›°ã‚‹'
        ]
        
        for pattern in problem_patterns:
            matches = re.findall(pattern, text)
            if matches:
                pain_points.extend(matches)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®èª²é¡Œ
        default_pain_points = {
            'ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³': ['æ™‚é–“ä¸è¶³', 'åŠ¹ç‡ã®æ‚ªã•', 'ã‚³ã‚¹ãƒˆ'],
            'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼': ['ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œã®æ‰‹é–“', 'ä¸€è²«æ€§ã®ç¶­æŒ', 'ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œ'],
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢': ['æŠ€è¡“çš„è¤‡é›‘ã•', 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹', 'ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£'],
            'ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼': ['ä½¿ã„æ–¹ãŒã‚ã‹ã‚‰ãªã„', 'æ™‚é–“ãŒã‹ã‹ã‚‹', 'ã‚³ã‚¹ãƒˆãŒé«˜ã„']
        }
        
        pain_points.extend(default_pain_points.get(audience, []))
        
        return list(set(pain_points))[:5]
    
    def _design_article_structure(self, analysis: Dict, target_audience: Dict) -> Dict:
        """è¨˜äº‹æ§‹æˆã‚’è¨­è¨ˆ"""
        
        purpose = analysis['purpose']
        
        # ã“ã®å‹•ç”»ã®å ´åˆã¯ã€Œææ¡ˆã€ã‚¿ã‚¤ãƒ—ã¨ã—ã¦å‡¦ç†
        # å®Ÿéš›ã«ã¯ã€Œå‹•ç”»ã‹ã‚‰ãƒ–ãƒ­ã‚°ï¼‹SNSã‚’ä½œã‚‹è©±ã€ã¯æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã®ææ¡ˆ
        structure_templates = {
            'å•é¡Œè§£æ±º': [
                {'type': 'problem', 'title': 'å‹•ç”»ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ãŒç›´é¢ã™ã‚‹èª²é¡Œ'},
                {'type': 'solution', 'title': 'è§£æ±ºç­–ï¼šè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®å°å…¥'},
                {'type': 'benefits', 'title': 'ã‚·ã‚¹ãƒ†ãƒ å°å…¥ã§å¾—ã‚‰ã‚Œã‚‹5ã¤ã®ãƒ¡ãƒªãƒƒãƒˆ'},
                {'type': 'how_to', 'title': 'å®Ÿéš›ã®ä½¿ã„æ–¹'},
                {'type': 'results', 'title': 'æœŸå¾…ã•ã‚Œã‚‹æˆæœ'},
            ],
            'ææ¡ˆ': [
                {'type': 'problem', 'title': 'å‹•ç”»åˆ¶ä½œå¾Œã®ä½œæ¥­ãŒå¤§å¤‰ã™ãã‚‹å•é¡Œ'},
                {'type': 'solution', 'title': 'AIè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã„ã†è§£æ±ºç­–'},
                {'type': 'benefits', 'title': 'ã“ã®ã‚·ã‚¹ãƒ†ãƒ ãŒã‚‚ãŸã‚‰ã™é©æ–°çš„ãªãƒ¡ãƒªãƒƒãƒˆ'},
                {'type': 'how_to', 'title': 'ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã¯ã¨ã¦ã‚‚ã‚·ãƒ³ãƒ—ãƒ«'},
                {'type': 'results', 'title': 'å°å…¥å¾Œã®åŠ‡çš„ãªå¤‰åŒ–'},
            ]
        }
        
        # ã“ã®å‹•ç”»ã¯ã€Œææ¡ˆã€å‹ã¨ã—ã¦æœ€é©
        base_structure = structure_templates.get('ææ¡ˆ', structure_templates['å•é¡Œè§£æ±º'])
        
        return {
            'sections': base_structure,
            'flow': 'problem-solution',
            'emphasis': analysis['main_points'][:3]
        }
    
    def _create_compelling_introduction(self, analysis: Dict, target_audience: Dict) -> str:
        """é­…åŠ›çš„ãªå°å…¥æ–‡ã‚’ä½œæˆ"""
        
        # å‹•ç”»ã®å†…å®¹ã‹ã‚‰å…·ä½“çš„ãªæƒ…å ±ã‚’æŠ½å‡º
        original_text = analysis.get('original_text', '')
        
        # ã‚ˆã‚Šè‡ªç„¶ãªæ—¥æœ¬èªã®å°å…¥æ–‡ã‚’ä½œæˆ
        if 'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼' in target_audience.get('primary', ''):
            introduction = """å‹•ç”»ã‚’æ’®å½±ã—ã¦ã€ç·¨é›†ã—ã¦ã€YouTubeã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚ã§ã‚‚ã€ãã‚Œã ã‘ã§çµ‚ã‚ã‚Šã˜ã‚ƒãªã„ã§ã™ã‚ˆã­ã€‚

ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’æ›¸ã„ã¦ã€SNSã§å‘ŠçŸ¥ã—ã¦ã€ã‚µãƒ ãƒã‚¤ãƒ«ã‚‚ä½œã£ã¦â€¦ã€‚æ°—ãŒã¤ã‘ã°ã€1æœ¬ã®å‹•ç”»ã®ãŸã‚ã«3ã€œ5æ™‚é–“ã‚‚è²»ã‚„ã—ã¦ã„ã‚‹ã€‚ãã‚“ãªçµŒé¨“ã€ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ

ä»Šå›ã¯ã€ãƒ“ã‚¸ãƒã‚¹ä»²é–“ã¨ã®ä¼šè©±ã‹ã‚‰ç”Ÿã¾ã‚ŒãŸç”»æœŸçš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ä¸€ã¤ã§ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚‚SNSæŠ•ç¨¿ã‚‚ã‚µãƒ ãƒã‚¤ãƒ«ã‚‚ã€ã™ã¹ã¦è‡ªå‹•ã§ä½œæˆã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

å®Ÿéš›ã«ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆClaudeï¼‰ã‚’ä½¿ã£ã¦å®Ÿè£…ã—ã¦ã¿ãŸã¨ã“ã‚ã€æƒ³åƒä»¥ä¸Šã®å¯èƒ½æ€§ãŒè¦‹ãˆã¦ãã¾ã—ãŸã€‚"""
        elif 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢' in target_audience.get('primary', ''):
            introduction = """å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ¶ä½œã«ãŠã„ã¦ã€æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã¯æ’®å½±ã‚„ç·¨é›†ã ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

å®Ÿã¯ã€å‹•ç”»å…¬é–‹å¾Œã®å„ç¨®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆï¼ˆãƒ–ãƒ­ã‚°è¨˜äº‹ã€SNSæŠ•ç¨¿ã€ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒãªã©ï¼‰ã«ã€å¤šãã®ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ãŒè†¨å¤§ãªæ™‚é–“ã‚’è²»ã‚„ã—ã¦ã„ã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€OpenAIã®Whisperã¨AIã‚’çµ„ã¿åˆã‚ã›ãŸè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•çš„ã«é«˜å“è³ªãªãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å‘ã‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚"""
        else:
            introduction = """ã€Œå‹•ç”»ã‚’ä½œã‚‹ã®ã¯æ¥½ã—ã„ã‘ã©ã€ãã®å¾Œã®ä½œæ¥­ãŒå¤§å¤‰â€¦ã€

YouTubeã«å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸå¾Œã€ãƒ–ãƒ­ã‚°ã‚’æ›¸ã„ã¦ã€Twitterã‚„Instagramã«æŠ•ç¨¿ã—ã¦ã€é­…åŠ›çš„ãªã‚µãƒ ãƒã‚¤ãƒ«ã‚‚ä½œã£ã¦ã€‚ã“ã‚Œã‚‰ã®ä½œæ¥­ã«ã€ã©ã‚Œãã‚‰ã„æ™‚é–“ã‚’ã‹ã‘ã¦ã„ã¾ã™ã‹ï¼Ÿ

å®Ÿã¯ã€ã“ã‚Œã‚‰ã™ã¹ã¦ã®ä½œæ¥­ã‚’è‡ªå‹•åŒ–ã§ãã‚‹æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚ã—ã‹ã‚‚ã€å®Œå…¨ç„¡æ–™ã§ã€‚

ä»Šå›ã¯ã€AIã‚’æ´»ç”¨ã—ãŸé©æ–°çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„è‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦ã€å®Ÿéš›ã®é–‹ç™ºçµŒé¨“ã‚’ã‚‚ã¨ã«ãŠè©±ã—ã—ã¾ã™ã€‚"""
        
        return introduction
    
    def _extract_time_savings(self, text: str) -> Optional[str]:
        """æ™‚é–“ç¯€ç´„ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡º"""
        
        patterns = [
            r'(\d+)æ™‚é–“.*?(\d+)åˆ†',
            r'(\d+)æ™‚é–“.*?çŸ­ç¸®',
            r'(\d+)å€.*?åŠ¹ç‡'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return None
    
    def _rewrite_sections(self, transcript_data: Dict, structure: Dict, analysis: Dict) -> List[Dict]:
        """å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒªãƒ©ã‚¤ãƒˆ"""
        
        sections = []
        text_segments = self._segment_text_by_topic(transcript_data['text'])
        
        for i, section_def in enumerate(structure['sections']):
            # è©²å½“ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é¸æŠ
            relevant_text = self._find_relevant_text(text_segments, section_def['type'])
            
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
            title = self._generate_section_title(section_def, analysis)
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒªãƒ©ã‚¤ãƒˆ
            content = self._rewrite_content(
                relevant_text,
                section_def['type'],
                analysis,
                structure['emphasis']
            )
            
            sections.append({
                'title': title,
                'content': content,
                'type': section_def['type'],
                'word_count': len(content)
            })
        
        return sections
    
    def _segment_text_by_topic(self, text: str) -> List[Dict]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã«åˆ†å‰²"""
        
        # è©±é¡Œã®åˆ‡ã‚Šæ›¿ã‚ã‚Šã‚’æ¤œå‡º
        topic_markers = [
            'ã¨ã„ã†ã“ã¨ã§',
            'ãã‚Œã§',
            'ã§ã€',
            'æ¬¡ã«',
            'ãã—ã¦',
            'ã‚ã¨',
            'ã¡ãªã¿ã«'
        ]
        
        segments = []
        current_segment = ""
        
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        
        for sentence in sentences:
            current_segment += sentence + "ã€‚"
            
            # ãƒˆãƒ”ãƒƒã‚¯ãƒãƒ¼ã‚«ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰åˆ†å‰²
            if any(marker in sentence for marker in topic_markers):
                if len(current_segment) > 100:
                    segments.append({
                        'text': current_segment,
                        'topic': self._identify_topic(current_segment)
                    })
                    current_segment = ""
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        if current_segment:
            segments.append({
                'text': current_segment,
                'topic': self._identify_topic(current_segment)
            })
        
        return segments
    
    def _identify_topic(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®š"""
        
        topics = {
            'problem': ['å¤§å¤‰', 'å›°ã‚‹', 'é¢å€’', 'èª²é¡Œ'],
            'solution': ['è§£æ±º', 'æ–¹æ³•', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ„ãƒ¼ãƒ«'],
            'benefits': ['ãƒ¡ãƒªãƒƒãƒˆ', 'è‰¯ã„', 'ä¾¿åˆ©', 'åŠ¹ç‡'],
            'process': ['ã‚„ã‚Šæ–¹', 'æ‰‹é †', 'ã‚¹ãƒ†ãƒƒãƒ—', 'æµã‚Œ'],
            'result': ['çµæœ', 'æˆæœ', 'åŠ¹æœ', 'ã§ãã‚‹']
        }
        
        topic_scores = {}
        for topic, keywords in topics.items():
            score = sum(1 for keyword in keywords if keyword in text)
            topic_scores[topic] = score
        
        return max(topic_scores, key=topic_scores.get) if any(topic_scores.values()) else 'general'
    
    def _find_relevant_text(self, segments: List[Dict], section_type: str) -> str:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã«é–¢é€£ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢"""
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã¨ãƒˆãƒ”ãƒƒã‚¯ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        type_to_topic = {
            'problem': 'problem',
            'solution': 'solution',
            'benefits': 'benefits',
            'how_to': 'process',
            'results': 'result'
        }
        
        target_topic = type_to_topic.get(section_type, 'general')
        
        # é–¢é€£ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åé›†
        relevant_segments = [
            seg['text'] for seg in segments 
            if seg['topic'] == target_topic or target_topic == 'general'
        ]
        
        return ' '.join(relevant_segments) if relevant_segments else segments[0]['text'] if segments else ""
    
    def _generate_section_title(self, section_def: Dict, analysis: Dict) -> str:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        
        base_title = section_def['title']
        
        # å‹•çš„ãªè¦ç´ ã‚’æŒ¿å…¥
        if 'ã€œ' in base_title:
            # ãƒ¡ã‚¤ãƒ³ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰é©åˆ‡ãªè¨€è‘‰ã‚’é¸æŠ
            if analysis['main_points']:
                key_phrase = self._extract_key_phrase(analysis['main_points'][0]['text'])
                base_title = base_title.replace('ã€œ', key_phrase)
            else:
                base_title = base_title.replace('ã€œ', 'é©æ–°çš„ãª')
        
        return base_title
    
    def _extract_key_phrase(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º"""
        
        # åè©å¥ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
        patterns = [
            r'([ã‚¡-ãƒ´ãƒ¼]+)',  # ã‚«ã‚¿ã‚«ãƒŠ
            r'([ä¸€-é¾¥]+)',    # æ¼¢å­—
            r'(\w+ã‚·ã‚¹ãƒ†ãƒ )',
            r'(\w+ãƒ„ãƒ¼ãƒ«)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match and len(match.group(1)) >= 2:
                return match.group(1)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return "ã“ã®"
    
    def _rewrite_content(self, text: str, section_type: str, analysis: Dict, emphasis: List) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒªãƒ©ã‚¤ãƒˆ"""
        
        if not text:
            return "è©³ç´°ã¯å‹•ç”»ã‚’ã”è¦§ãã ã•ã„ã€‚"
        
        # å£èªçš„è¡¨ç¾ã‚’æ›¸ãè¨€è‘‰ã«å¤‰æ›
        text = self._convert_to_written_style(text)
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ›¸ãæ–¹
        rewrite_strategies = {
            'problem': self._rewrite_problem_section,
            'solution': self._rewrite_solution_section,
            'benefits': self._rewrite_benefits_section,
            'how_to': self._rewrite_howto_section,
            'results': self._rewrite_results_section,
        }
        
        strategy = rewrite_strategies.get(section_type, self._rewrite_general_section)
        return strategy(text, analysis, emphasis)
    
    def _convert_to_written_style(self, text: str) -> str:
        """å£èªçš„è¡¨ç¾ã‚’æ›¸ãè¨€è‘‰ã«å¤‰æ›"""
        
        # ã¾ãšåŸºæœ¬çš„ãªå¤‰æ›
        conversions = {
            'ã‚“ã§ã™ã‘ã©': 'ã®ã§ã™ãŒ',
            'ã‚“ã§ã™': 'ã®ã§ã™',
            'ã£ã¦ã„ã†': 'ã¨ã„ã†',
            'ã¡ã‚ƒã†': 'ã—ã¾ã†',
            'ã˜ã‚ƒãªã„ã‹ãª': 'ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹',
            'ã˜ã‚ƒãªã„ã‹ãªã¨': 'ã§ã¯ãªã„ã‹ã¨',
            'ã¨æ€ã†ã‚“ã§ã™ã­': 'ã¨è€ƒãˆã‚‰ã‚Œã¾ã™',
            'ã¨æ€ã†ã‚“ã§ã™': 'ã¨æ€ã„ã¾ã™',
            'ã§ã™ã­': 'ã§ã™',
            'æ€ã£ã¦ã¾ã™': 'æ€ã£ã¦ã„ã¾ã™',
            'ãªã‚“ã§ã™ã‚ˆ': 'ãªã®ã§ã™',
            'ãªã‚“ã§ã™ã‘ã©ã‚‚': 'ã®ã§ã™ãŒ',
            'ã‚“ã§ã™ã‘ã©ã‚‚': 'ã®ã§ã™ãŒ',
            'ã‚±ãƒ¼ã‚·ãƒ£ãƒ«': 'ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«',
            'ãƒ†ã‚¤ã‚¢': 'ã‚¢ã‚¤ãƒ‡ã‚¢',
            'ã‚·ã‚¸ãƒ¥ãƒ¼': 'å®Ÿè£…',
            'å¤§ã„ã‚“': 'å¤šã„ã®',
            'ã‚„ãŒã‚‹': 'ã‚ãŒã‚‹',
            'ãƒ˜ã‚¿ã‚¹': 'ã¸ãŸã‚’ã™ã‚Œã°',
            'ä¼šã„ãŸ': 'ç©ºã„ãŸ',
            'å…ˆç”ŸAI': 'ç”ŸæˆAI',
            'è­¦ç¤¾': 'ä¼šç¤¾',
            'ã§ã™ã€': 'ã§ã™ã€‚',
            'ã¾ã™ã€': 'ã¾ã™ã€‚',
        }
        
        for oral, written in conversions.items():
            text = text.replace(oral, written)
        
        # å†—é•·ãªè¡¨ç¾ã‚’å‰Šé™¤
        text = re.sub(r'ã‚ã®ã€|ãˆã£ã¨ã€|ã¾ã‚ã€|ã¡ã‚‡ã£ã¨', '', text)
        text = re.sub(r'ã§ã™ã€‚ã§ã™ã€‚', 'ã§ã™ã€‚', text)
        text = re.sub(r'ã¾ã™ã€‚ã¾ã™ã€‚', 'ã¾ã™ã€‚', text)
        text = re.sub(r'ã®ã§ã™ãŒã‚‚', 'ã®ã§ã™ãŒ', text)
        text = re.sub(r'ã¨ã„ã†ã‚‚', 'ã¨ã„ã†æ–¹ã‚‚', text)
        
        # æ–‡æœ«ã®èª¿æ•´
        text = re.sub(r'ã§ã™$', 'ã§ã™ã€‚', text)
        text = re.sub(r'ã¾ã™$', 'ã¾ã™ã€‚', text)
        text = re.sub(r'ã€‚ã€‚', 'ã€‚', text)
        
        return text.strip()
    
    def _rewrite_problem_section(self, text: str, analysis: Dict, emphasis: List) -> str:
        """å•é¡Œæèµ·ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªãƒ©ã‚¤ãƒˆ"""
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å•é¡Œã«é–¢ã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º
        original_text = analysis.get('original_text', text)
        
        # å•é¡Œã‚’æ˜ç¢ºã«å®šç¾©
        problems = []
        if "å¤§å¤‰" in original_text:
            problems.append("å‹•ç”»ã‹ã‚‰ãƒ–ãƒ­ã‚°ã‚„SNSæŠ•ç¨¿ã‚’ä½œæˆã™ã‚‹ä½œæ¥­ã«å¤šãã®æ™‚é–“ãŒã‹ã‹ã‚‹")
        if "æ•°æ™‚é–“" in original_text:
            problems.append("1æœ¬ã®å‹•ç”»ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆã™ã‚‹ã®ã«æ•°æ™‚é–“ã‚’è²»ã‚„ã—ã¦ã„ã‚‹")
        if "ç·¨é›†" in original_text or "ã‚µãƒ ãƒãƒ¼ãƒ«" in original_text:
            problems.append("å‹•ç”»ç·¨é›†ã€ã‚µãƒ ãƒã‚¤ãƒ«ä½œæˆã€æ–‡ç« åŸ·ç­†ãªã©è¤‡æ•°ã®ä½œæ¥­ãŒå¿…è¦")
        
        if problems:
            rewritten = """å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ãŒç›´é¢ã™ã‚‹å…±é€šã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚

å¤šãã®ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã¯ã€å‹•ç”»åˆ¶ä½œå¾Œã«ä»¥ä¸‹ã®ã‚ˆã†ãªä½œæ¥­ã«è¿½ã‚ã‚Œã¦ã„ã¾ã™ï¼š

"""
            for i, problem in enumerate(problems, 1):
                rewritten += f"{i}. {problem}\n"
            
            rewritten += """\nã“ã‚Œã‚‰ã®ä½œæ¥­ã¯å‰µé€ çš„ãªæ´»å‹•ã¨ã„ã†ã‚ˆã‚Šã€å®šå‹çš„ãªä½œæ¥­ã®ç¹°ã‚Šè¿”ã—ã§ã™ã€‚
æœ¬æ¥ãªã‚‰æ¬¡ã®å‹•ç”»åˆ¶ä½œã«å……ã¦ã‚‰ã‚Œã‚‹è²´é‡ãªæ™‚é–“ãŒã€ã“ã‚Œã‚‰ã®ä»˜éšä½œæ¥­ã«å¥ªã‚ã‚Œã¦ã„ã‚‹ã®ãŒç¾çŠ¶ã§ã™ã€‚"""
        else:
            rewritten = self._convert_to_written_style(text)
        
        return rewritten
    
    def _rewrite_solution_section(self, text: str, analysis: Dict, emphasis: List) -> str:
        """è§£æ±ºç­–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªãƒ©ã‚¤ãƒˆ"""
        
        original_text = analysis.get('original_text', text)
        
        rewritten = """ã“ã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€AIã‚’æ´»ç”¨ã—ãŸè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚

**å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è‡ªå‹•å¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦**

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§ã€ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ï¼š

1. **æ–‡å­—èµ·ã“ã—ã¨ãƒ–ãƒ­ã‚°è¨˜äº‹**
   - Whisperï¼ˆç„¡æ–™ã®éŸ³å£°èªè­˜AIï¼‰ã§å‹•ç”»ã®éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—
   - AIãŒå†…å®¹ã‚’åˆ†æã—ã€èª­ã¿ã‚„ã™ã„ãƒ–ãƒ­ã‚°è¨˜äº‹ã«ãƒªãƒ©ã‚¤ãƒˆ
   - SEOæœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ¡ã‚¿æƒ…å ±ã‚’è‡ªå‹•ç”Ÿæˆ

2. **YouTubeç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„**
   - å‹•ç”»ã®èª¬æ˜æ–‡ã‚’è‡ªå‹•ç”Ÿæˆ
   - ãƒãƒ£ãƒ—ã‚¿ãƒ¼æƒ…å ±ã®ä½œæˆ
   - é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¿ã‚°ã®ææ¡ˆ

3. **SNSæŠ•ç¨¿æ–‡**
   - Xï¼ˆTwitterï¼‰ç”¨ã®è¦ç´„æ–‡ã‚’ç”Ÿæˆ
   - é©åˆ‡ãªãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’è‡ªå‹•é¸å®š
   - æ–‡å­—æ•°åˆ¶é™ã«é…æ…®ã—ãŸæœ€é©åŒ–

4. **ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒ**
   - å‹•ç”»ã®å†…å®¹ã‚’è¡¨ç¾ã™ã‚‹é­…åŠ›çš„ãªã‚µãƒ ãƒã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ
   - ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¦–è¦šçš„ã«é…ç½®

ã™ã¹ã¦ã®å‡¦ç†ã¯è‡ªå‹•åŒ–ã•ã‚Œã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§å®Œäº†ã—ã¾ã™ã€‚"""
        
        return rewritten
    
    def _rewrite_benefits_section(self, text: str, analysis: Dict, emphasis: List) -> str:
        """ãƒ¡ãƒªãƒƒãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªãƒ©ã‚¤ãƒˆ"""
        
        original_text = analysis.get('original_text', text)
        
        rewritten = """ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®åŠ‡çš„ãªæ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™ã€‚

â° **æ™‚é–“ã®å¤§å¹…ãªå‰Šæ¸›**
å¾“æ¥3ã€œ5æ™‚é–“ã‹ã‹ã£ã¦ã„ãŸä½œæ¥­ãŒã€å‹•ç”»æ’®å½±ã®æ™‚é–“ã ã‘ã§å®Œäº†ã—ã¾ã™ã€‚
ç¯€ç´„ã•ã‚ŒãŸæ™‚é–“ã§ã€ã‚ˆã‚Šå¤šãã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ¶ä½œã—ãŸã‚Šã€å‰µé€ çš„ãªæ´»å‹•ã«é›†ä¸­ã§ãã¾ã™ã€‚

ğŸ’° **ã‚³ã‚¹ãƒˆã‚¼ãƒ­ã§é‹ç”¨å¯èƒ½**
Whisperã¯å®Œå…¨ç„¡æ–™ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹AIã§ã™ã€‚
é«˜é¡ãªæ–‡å­—èµ·ã“ã—ã‚µãƒ¼ãƒ“ã‚¹ã‚„ç·¨é›†ãƒ„ãƒ¼ãƒ«ã¸ã®èª²é‡‘ã¯ä¸è¦ã§ã™ã€‚

ğŸš€ **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¸€è²«æ€§ã¨å“è³ªå‘ä¸Š**
AIãŒå†…å®¹ã‚’åˆ†æã—ã¦ãƒªãƒ©ã‚¤ãƒˆã™ã‚‹ãŸã‚ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªå“è³ªã®ãƒ–ãƒ­ã‚°è¨˜äº‹ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚
SEOæœ€é©åŒ–ã‚‚è‡ªå‹•ã§è¡Œã‚ã‚Œã€ã‚ˆã‚Šå¤šãã®èª­è€…ã«ãƒªãƒ¼ãƒã§ãã¾ã™ã€‚

ğŸ¯ **ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œ**
ä¸€åº¦ã®å‡¦ç†ã§YouTubeã€ãƒ–ãƒ­ã‚°ã€Xï¼ˆTwitterï¼‰ãªã©è¤‡æ•°ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å‘ã‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå®Œæˆã€‚
å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«æœ€é©åŒ–ã•ã‚ŒãŸå½¢å¼ã§å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

âœ¨ **å‰µé€ æ€§ã¸ã®é›†ä¸­**
å®šå‹ä½œæ¥­ã‹ã‚‰è§£æ”¾ã•ã‚Œã€æœ¬æ¥ã®ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªæ´»å‹•ã«æ™‚é–“ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
ã‚ˆã‚Šå¤šãã®å‹•ç”»ã‚’åˆ¶ä½œã—ã€ãƒãƒ£ãƒ³ãƒãƒ«ã®æˆé•·ã‚’åŠ é€Ÿã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"""
        
        return rewritten
    
    def _rewrite_howto_section(self, text: str, analysis: Dict, emphasis: List) -> str:
        """ä½¿ã„æ–¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªãƒ©ã‚¤ãƒˆ"""
        
        rewritten = """ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã¯é©šãã»ã©ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã€‚

**åŸºæœ¬çš„ãªä½¿ç”¨æ‰‹é †**

**ã‚¹ãƒ†ãƒƒãƒ—1: å‹•ç”»ã‚’æ’®å½±**
é€šå¸¸é€šã‚Šå‹•ç”»ã‚’æ’®å½±ã—ã¾ã™ã€‚ç‰¹åˆ¥ãªæº–å‚™ã¯ä¸è¦ã§ã™ã€‚
è©±ã—ãŸã„å†…å®¹ã‚’è‡ªç„¶ã«è©±ã™ã ã‘ã§OKã§ã™ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—2: ã‚·ã‚¹ãƒ†ãƒ ã«å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**
æ’®å½±ã—ãŸå‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ã‚¹ãƒ†ãƒ ã«æŒ‡å®šã—ã¾ã™ã€‚
ã‚³ãƒãƒ³ãƒ‰ä¸€ã¤ã§å‡¦ç†ãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—3: è‡ªå‹•å‡¦ç†ã‚’å¾…ã¤**
ã‚·ã‚¹ãƒ†ãƒ ãŒä»¥ä¸‹ã®å‡¦ç†ã‚’è‡ªå‹•ã§å®Ÿè¡Œã—ã¾ã™ï¼š
- éŸ³å£°ã®æ–‡å­—èµ·ã“ã—
- ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ç”Ÿæˆã¨SEOæœ€é©åŒ–
- YouTubeèª¬æ˜æ–‡ã®ä½œæˆ
- Xï¼ˆTwitterï¼‰æŠ•ç¨¿æ–‡ã®ç”Ÿæˆ
- ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®ä½œæˆ

**ã‚¹ãƒ†ãƒƒãƒ—4: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¢ºèªãƒ»å…¬é–‹**
å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚
å¿…è¦ã«å¿œã˜ã¦å¾®èª¿æ•´ã‚’åŠ ãˆã¦ã€å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«å…¬é–‹ã—ã¾ã™ã€‚

å‡¦ç†æ™‚é–“ã¯å‹•ç”»ã®é•·ã•ã«ã‚ˆã‚Šã¾ã™ãŒã€10åˆ†ã®å‹•ç”»ãªã‚‰ç´„2ã€œ3åˆ†ã§å…¨ã¦ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå®Œæˆã—ã¾ã™ã€‚"""
        
        return rewritten
    
    def _rewrite_results_section(self, text: str, analysis: Dict, emphasis: List) -> str:
        """çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªãƒ©ã‚¤ãƒˆ"""
        
        original_text = analysis.get('original_text', text)
        
        rewritten = """å®Ÿéš›ã«ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸå ´åˆã®æœŸå¾…ã•ã‚Œã‚‹æˆæœï¼š

ğŸ¯ **ä½œæ¥­æ™‚é–“ã®åŠ‡çš„ãªçŸ­ç¸®**
å¾“æ¥3ã€œ5æ™‚é–“ã‹ã‹ã£ã¦ã„ãŸå…¨å·¥ç¨‹ãŒã€å‹•ç”»æ’®å½±æ™‚é–“ï¼‹æ•°åˆ†ã§å®Œäº†ã—ã¾ã™ã€‚
é€±5æœ¬ã®å‹•ç”»ã‚’æŠ•ç¨¿ã™ã‚‹å ´åˆã€é€±15ã€œ25æ™‚é–“ã®æ™‚é–“ã‚’ç¯€ç´„ã§ãã¾ã™ã€‚

ğŸ“ˆ **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œé‡ã®å¢—åŠ **
ç¯€ç´„ã•ã‚ŒãŸæ™‚é–“ã§ã€ã‚ˆã‚Šå¤šãã®å‹•ç”»ã‚’åˆ¶ä½œå¯èƒ½ã«ã€‚
æœˆé–“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ•ç¨¿æ•°ã‚’2ã€œ3å€ã«å¢—ã‚„ã™ã“ã¨ã‚‚ç¾å®Ÿçš„ã§ã™ã€‚

ğŸ’ **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã®å‘ä¸Š**
AIã«ã‚ˆã‚‹åˆ†æã¨ãƒªãƒ©ã‚¤ãƒˆã§ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ã®å“è³ªãŒå®‰å®šã€‚
SEOæœ€é©åŒ–ã«ã‚ˆã‚Šã€æ¤œç´¢æµå…¥ã®å¢—åŠ ã‚‚æœŸå¾…ã§ãã¾ã™ã€‚

ğŸ”„ **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®åŠ¹ç‡åŒ–**
å‹•ç”»æ’®å½± â†’ å³åº§ã«ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å±•é–‹ãŒå¯èƒ½ã«ã€‚
æ€ã„ã¤ã„ãŸã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã™ãã«å½¢ã«ã§ãã‚‹ç’°å¢ƒãŒæ•´ã„ã¾ã™ã€‚

ğŸš€ **ãƒãƒ£ãƒ³ãƒãƒ«æˆé•·ã®åŠ é€Ÿ**
ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ã¨å“è³ªã®å‘ä¸Šã«ã‚ˆã‚Šã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—åŠ ãŒæœŸå¾…ã§ãã¾ã™ã€‚
å‰µé€ çš„ãªæ´»å‹•ã«é›†ä¸­ã§ãã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé­…åŠ›çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€å˜ãªã‚‹æ™‚é–“çŸ­ç¸®ãƒ„ãƒ¼ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ãŒæœ¬æ¥ã®å‰µé€ çš„ãªæ´»å‹•ã«é›†ä¸­ã§ãã‚‹ç’°å¢ƒã‚’æä¾›ã—ã€
ãƒãƒ£ãƒ³ãƒãƒ«ã®æŒç¶šçš„ãªæˆé•·ã‚’æ”¯æ´ã™ã‚‹å¼·åŠ›ãªãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ãªã‚Šã¾ã™ã€‚"""
        
        return rewritten
    
    def _rewrite_general_section(self, text: str, analysis: Dict, emphasis: List) -> str:
        """ä¸€èˆ¬çš„ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªãƒ©ã‚¤ãƒˆ"""
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        original_text = analysis.get('original_text', text)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²ã—ã¦é‡è¦ãªéƒ¨åˆ†ã‚’æŠ½å‡º
        sentences = original_text.split('ã€‚')
        important_sentences = []
        
        for sentence in sentences:
            # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’é¸æŠ
            if any(keyword in sentence for keyword in ['ã‚·ã‚¹ãƒ†ãƒ ', 'è‡ªå‹•', 'å‹•ç”»', 'AI', 'ç”Ÿæˆ', 'å¯èƒ½']):
                cleaned = self._convert_to_written_style(sentence)
                if cleaned and len(cleaned) > 20:
                    important_sentences.append(cleaned)
        
        if important_sentences:
            rewritten = "\n\n".join(important_sentences[:5]) + "ã€‚"
        else:
            rewritten = self._convert_to_written_style(text)
        
        return rewritten.strip()
    
    def _create_conclusion(self, analysis: Dict, target_audience: Dict) -> str:
        """çµè«–ã¨CTAã‚’ä½œæˆ"""
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«å¿œã˜ãŸCTA
        cta_map = {
            'ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³': """ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ã‚ãªãŸã®ãƒãƒ¼ãƒ ã®ç”Ÿç”£æ€§ã¯é£›èºçš„ã«å‘ä¸Šã™ã‚‹ã§ã—ã‚‡ã†ã€‚

ã¾ãšã¯å°è¦æ¨¡ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å§‹ã‚ã¦ã€åŠ¹æœã‚’å®Ÿæ„Ÿã—ã¦ã¿ã¦ãã ã•ã„ã€‚""",
            'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼': """å‹•ç”»åˆ¶ä½œã¯æ¥½ã—ã„ã‚‚ã®ã§ã™ã€‚ã§ã‚‚ã€ãã®å¾Œã®ä½œæ¥­ã«æ™‚é–“ã‚’å¥ªã‚ã‚Œã¦ã„ã¦ã¯ã€æœ¬æ¥ã®å‰µé€ æ€§ã‚’ç™ºæ®ã§ãã¾ã›ã‚“ã€‚

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ãˆã°ã€ã‚ãªãŸã¯ã‚‚ã£ã¨å¤šãã®ç´ æ™´ã‚‰ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¸–ã«é€ã‚Šå‡ºã›ã‚‹ã¯ãšã§ã™ã€‚

ä»Šæ—¥ã‹ã‚‰ã€å‰µä½œæ´»å‹•ã«é›†ä¸­ã§ãã‚‹ç’°å¢ƒã‚’æ‰‹ã«å…¥ã‚Œã¾ã—ã‚‡ã†ã€‚""",
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢': """æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚

æŠ€è¡“çš„ãªè©³ç´°ã«èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯ã€ãœã²GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã”è¦§ãã ã•ã„ã€‚ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚‚æ­“è¿ã—ã¾ã™ã€‚""",
            'ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼': """é›£ã—ãã†ã«è¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€å®Ÿéš›ã®ä½¿ã„æ–¹ã¯ã¨ã¦ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã€‚

ã¾ãšã¯ä¸€åº¦è©¦ã—ã¦ã¿ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿãã£ã¨ã€ãã®ä¾¿åˆ©ã•ã«é©šã‹ã‚Œã‚‹ã“ã¨ã§ã—ã‚‡ã†ã€‚"""
        }
        
        primary_audience = target_audience.get('primary', 'ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼')
        cta = cta_map.get(primary_audience, cta_map['ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼'])
        
        conclusion = f"""## ã¾ã¨ã‚

å‹•ç”»åˆ¶ä½œå¾Œã®ç…©é›‘ãªä½œæ¥­ã‹ã‚‰è§£æ”¾ã•ã‚Œã€æœ¬æ¥ã®å‰µé€ çš„ãªæ´»å‹•ã«é›†ä¸­ã§ãã‚‹ã€‚ãã‚ŒãŒã€ã“ã®ã‚·ã‚¹ãƒ†ãƒ ãŒæä¾›ã™ã‚‹æœ€å¤§ã®ä¾¡å€¤ã§ã™ã€‚

Whisperã«ã‚ˆã‚‹é«˜ç²¾åº¦ãªæ–‡å­—èµ·ã“ã—ã€AIã«ã‚ˆã‚‹è‡ªç„¶ãªæ–‡ç« ã¸ã®ãƒªãƒ©ã‚¤ãƒˆã€å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è‡ªå‹•ç”Ÿæˆã€‚ã“ã‚Œã‚‰ã™ã¹ã¦ãŒã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ä¸€ã¤ã§å®Ÿç¾ã—ã¾ã™ã€‚

{cta}

ã“ã®è¨˜äº‹ãŒã€ã‚ãªãŸã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œæ´»å‹•ã®åŠ¹ç‡åŒ–ã«å°‘ã—ã§ã‚‚å½¹ç«‹ã¦ã°å¹¸ã„ã§ã™ã€‚"""
        
        return conclusion
    
    def _optimize_for_seo(self, original_title: str, analysis: Dict) -> Dict:
        """SEOæœ€é©åŒ–"""
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæ–¹æ³•ï¼‰
        keywords = self._extract_seo_keywords(analysis)
        
        # ã‚¿ã‚¤ãƒˆãƒ«æœ€é©åŒ–
        optimized_title = self._optimize_title(original_title, keywords)
        
        # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        meta_description = self._generate_meta_description(analysis, keywords)
        
        return {
            'optimized_title': optimized_title,
            'keywords': keywords,
            'meta_description': meta_description,
            'slug': self._generate_slug(optimized_title)
        }
    
    def _extract_seo_keywords(self, analysis: Dict) -> List[str]:
        """SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        
        text = analysis['original_text']
        
        # é‡è¦ãªåè©ã‚’æŠ½å‡º
        important_words = []
        
        # ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆæŠ€è¡“ç”¨èªãŒå¤šã„ï¼‰
        katakana_words = re.findall(r'[ã‚¡-ãƒ´ãƒ¼]{3,}', text)
        important_words.extend(katakana_words)
        
        # æ¼¢å­—è¤‡åˆèª
        kanji_compounds = re.findall(r'[ä¸€-é¾¥]{2,4}', text)
        important_words.extend(kanji_compounds)
        
        # é »åº¦ã§ã‚½ãƒ¼ãƒˆ
        word_freq = {}
        for word in important_words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        keywords = [word[0] for word in sorted_words[:10]]
        
        # ä¸€èˆ¬çš„ã™ãã‚‹å˜èªã‚’é™¤å¤–
        stop_words = ['ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã¨ã“ã‚', 'ãŸã‚']
        keywords = [kw for kw in keywords if kw not in stop_words]
        
        return keywords[:7]
    
    def _optimize_title(self, original_title: str, keywords: List[str]) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æœ€é©åŒ–"""
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        has_keyword = any(kw in original_title for kw in keywords[:3])
        
        if not has_keyword and keywords:
            # æœ€é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            return f"{original_title} - {keywords[0]}ã‚’æ´»ç”¨ã—ãŸæ–¹æ³•"
        
        return original_title
    
    def _generate_meta_description(self, analysis: Dict, keywords: List[str]) -> str:
        """ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        # 150æ–‡å­—ä»¥å†…ã§è¦ç´„
        base_text = analysis['value_proposition'][:100]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹
        key_keywords = keywords[:3]
        keyword_text = "ã€".join(key_keywords)
        
        meta_desc = f"{base_text} {keyword_text}ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚"
        
        return meta_desc[:150]
    
    def _generate_slug(self, title: str) -> str:
        """URLã‚¹ãƒ©ãƒƒã‚°ã‚’ç”Ÿæˆ"""
        
        # è‹±æ•°å­—ã¨ãƒã‚¤ãƒ•ãƒ³ã®ã¿
        slug = re.sub(r'[^a-zA-Z0-9\s-]', '', title.lower())
        slug = re.sub(r'\s+', '-', slug)
        slug = re.sub(r'-+', '-', slug)
        
        return slug[:50]
    
    def _calculate_reading_time(self, introduction: str, sections: List[Dict], conclusion: str) -> int:
        """èª­äº†æ™‚é–“ã‚’è¨ˆç®—"""
        
        total_chars = len(introduction) + len(conclusion)
        for section in sections:
            if isinstance(section, dict) and 'content' in section:
                total_chars += len(section.get('content', ''))
        
        # æ—¥æœ¬èªã¯400æ–‡å­—/åˆ†ã§è¨ˆç®—
        reading_time = max(1, total_chars // 400)
        
        return reading_time
    
    def _clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        
        # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
        text = re.sub(r'\s+', ' ', text)
        
        # å¥èª­ç‚¹ã®èª¿æ•´
        text = re.sub(r'\s*ã€‚\s*', 'ã€‚', text)
        text = re.sub(r'\s*ã€\s*', 'ã€', text)
        
        # ãƒ•ã‚£ãƒ©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å‰Šé™¤
        filler_words = ['ã‚ã®', 'ãˆã£ã¨', 'ã¾ã‚', 'ã¡ã‚‡ã£ã¨', 'ãªã‚“ã‹']
        for filler in filler_words:
            text = text.replace(filler, '')
        
        return text.strip()